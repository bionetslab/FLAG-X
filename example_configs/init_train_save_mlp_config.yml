
# flagx init-train-save --config ./configs/init_train_save_mlp_config.yml

save_dir: "./results/test_cli/mlp"
pipeline_filename: "gating_pipeline_mlp_trained.pkl"

train_data_file_path: "./tests/test_data"
train_data_file_names: null  # Use all files in the dir for training
train_data_file_type: "fcs"

channels: ["FS INT", "SS INT", "16-FITC", "56-PE", "3-ECD", "4-PC7", "19-APC", "14-APC700", "8-PB", "45-CO"]
label_key: "population"

compensate: false

channel_names_alignment_kwargs:
  reference_channel_names: 0  # Use the 1st file that is loaded as reference for the channel names

relabel_data_kwargs:  # No relabeling, keep the original labels ({'old_to_new_label_mapping': Dict, 'new_label_key': str})
  old_to_new_label_mapping:
    1: 0
    2: 1
    3: 2
    4: 3
    5: 4
    6: 5
    7: 6
    8: 7
  new_label_key: 'population_relabeled'

preprocessing_kwargs:
  flavour: "log10_w_custom_cutoffs"
  flavour_kwargs:
    cutoffs:
      FS INT: 100000
      SS INT: 20000
      16-FITC: 250
      56-PE: 450
      3-ECD: 700
      4-PC7: 1200
      19-APC: 1700
      14-APC700: 900
      8-PB: 450
      45-CO: 500

downsampling_kwargs:
  target_num_events: 10000
  stratified: true

gating_method: "mlp"
gating_method_kwargs:
  layer_sizes: [64, 32, 16]
  n_epochs: 6
  data_loader_params:
    batch_size: 128
    shuffle: true
    num_workers: 1
  device: null  # Use cuda if available, else cpu
  verbosity: 2
prediction_threshold: null  # Use default (argmax across class probabilities for multiclass case, threshold 0.5 for binary case)

verbosity: 1







